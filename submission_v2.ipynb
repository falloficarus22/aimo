{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO3 Consensus Voting Submission - Optimized for 2x T4 GPUs\n",
    "\n",
    "Competition: AI Mathematical Olympiad - Progress Prize 3\n",
    "\n",
    "Strategy: Generate 3 solutions with consensus voting, model parallel on 2x T4\n",
    "\n",
    "**Important**: This notebook uses the Qwen2.5-32B-Instruct quantized model from a Kaggle dataset. Make sure to add the dataset:\n",
    "- Dataset: `gmhost/qwen2-5-32b-instruct-quant` (17GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Note: transformers, torch, and accelerate are pre-installed in Kaggle environment\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import glob\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The model path points to the quantized Kaggle dataset for offline access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for consensus voting system.\"\"\"\n",
    "\n",
    "    # Model configuration - Qwen2.5-32B-Instruct quantized from Kaggle dataset\n",
    "    # Using local Kaggle dataset: gmhost/qwen2-5-32b-instruct-quant\n",
    "    MODEL_NAME = \"/kaggle/input/qwen2-5-32b-instruct-quant/\"\n",
    "    MAX_NEW_TOKENS = 4096\n",
    "    BASE_TEMPERATURE = 0.1\n",
    "    TOP_P = 0.95\n",
    "\n",
    "    # 2x T4 GPU Configuration\n",
    "    USE_FP16 = True  # Use fp16 instead of 4-bit for speed\n",
    "    DEVICE_MAP = \"auto\"  # Automatically split across GPUs\n",
    "\n",
    "    # Consensus configuration\n",
    "    NUM_SOLUTIONS = 3  # Number of solutions for voting\n",
    "    TEMPERATURES = [0.1, 0.3, 0.5]  # Diversity through temperature\n",
    "\n",
    "    # Constraints\n",
    "    MIN_ANSWER = 0\n",
    "    MAX_ANSWER = 99999\n",
    "    TIMEOUT_PER_PROBLEM = 300  # 5 minutes max\n",
    "\n",
    "\n",
    "print(f\"âœ“ Configuration:\")\n",
    "print(f\"  Model: {Config.MODEL_NAME}\")\n",
    "print(f\"  Consensus solutions: {Config.NUM_SOLUTIONS}\")\n",
    "print(f\"  Temperatures: {Config.TEMPERATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_PROBLEMS = \"\"\"\n",
    "Here are examples of AIMO3 problems and their solutions:\n",
    "\n",
    "Example 1 (Geometry):\n",
    "Problem: Let $ABC$ be an acute-angled triangle with integer side lengths and $AB<AC$. Points $D$ and $E$ lie on segments $BC$ and $AC$, respectively, such that $AD=AE=AB$. Line $DE$ intersects $AB$ at $X$. Circles $BXD$ and $CED$ intersect for the second time at $Y \\neq D$. Suppose that $Y$ lies on line $AD$. There is a unique such triangle with minimal perimeter. This triangle has side lengths $a=BC$, $b=CA$, and $c=AB$. Find the remainder when $abc$ is divided by $10^{5}$.\n",
    "Answer: 336\n",
    "\n",
    "Example 2 (Number Theory):\n",
    "Problem: Define a function $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$ by $f(n) = \\sum_{i = 1}^n \\sum_{j = 1}^n j^{1024} \\left\\lfloor\\frac1j + \\frac{n-i}{n}\\right\\rfloor$. Let $M=2 \\cdot 3 \\cdot 5 \\cdot 7 \\cdot 11 \\cdot 13$ and let $N = f{(M^{15})} - f{(M^{15}-1)}$. Let $k$ be the largest non-negative integer such that $2^k$ divides $N$. What is the remainder when $2^k$ is divided by $5^7$?\n",
    "Answer: 32951\n",
    "\n",
    "Example 3 (Algebra):\n",
    "Problem: Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ``If we each added the number of sweets we're holding to our (positive integer) age, my answer would be double yours. If we took the product, then my answer would be four times yours.'' Bob replies: ``Why don't you give me five of your sweets because then both our sum and product would be equal.'' What is the product of Alice and Bob's ages?\n",
    "Answer: 50\n",
    "\n",
    "Example 4 (Combinatorics):\n",
    "Problem: A $500 \\times 500$ square is divided into $k$ rectangles, each having integer side lengths. Given that no two of these rectangles have the same perimeter, the largest possible value of $k$ is $\\mathcal{K}$. What is the remainder when $k$ is divided by $10^{5}$?\n",
    "Answer: 520\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ“ Reference problems loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Model Manager\n",
    "\n",
    "Loads quantized model from local Kaggle dataset with `local_files_only=True` for offline inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUModelManager:\n",
    "    \"\"\"\n",
    "    Manages model loading across 2x T4 GPUs.\n",
    "    Uses automatic device mapping for optimal memory distribution.\n",
    "    Loads from local Kaggle dataset for offline inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = None\n",
    "        self.num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load model across available GPUs from local dataset.\"\"\"\n",
    "        if self.model is not None:\n",
    "            return self\n",
    "\n",
    "        print(f\"Loading {Config.MODEL_NAME}...\")\n",
    "        print(f\"Available GPUs: {self.num_gpus}\")\n",
    "\n",
    "        # Check if model path exists\n",
    "        if not os.path.exists(Config.MODEL_NAME):\n",
    "            print(f\"ERROR: Model path does not exist: {Config.MODEL_NAME}\")\n",
    "            # Try to find the actual dataset path\n",
    "            possible_paths = glob.glob(\"/kaggle/input/*/\")\n",
    "            print(f\"Available dataset paths: {possible_paths}\")\n",
    "            raise FileNotFoundError(f\"Model path not found: {Config.MODEL_NAME}\")\n",
    "        \n",
    "        print(f\"âœ“ Model path exists: {Config.MODEL_NAME}\")\n",
    "        # List files in the directory\n",
    "        files = os.listdir(Config.MODEL_NAME)\n",
    "        print(f\"Files in model directory: {files[:10]}...\")  # Show first 10 files\n",
    "\n",
    "        # Set memory limits per GPU (T4 has ~16GB each)\n",
    "        max_memory = {i: \"15GiB\" for i in range(self.num_gpus)}\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer from local files\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                Config.MODEL_NAME, trust_remote_code=True, local_files_only=True\n",
    "            )\n",
    "            print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "            # Load model from local files with automatic device mapping\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                Config.MODEL_NAME,\n",
    "                torch_dtype=torch.float16 if Config.USE_FP16 else torch.float32,\n",
    "                device_map=Config.DEVICE_MAP,\n",
    "                max_memory=max_memory,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "            print(\"âœ“ Model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading model: {e}\")\n",
    "            print(f\"Model path content:\")\n",
    "            for f in os.listdir(Config.MODEL_NAME):\n",
    "                print(f\"  - {f}\")\n",
    "            raise\n",
    "\n",
    "        # Check device allocation\n",
    "        if hasattr(self.model, \"hf_device_map\"):\n",
    "            print(f\"âœ“ Model sharded across devices: {self.model.hf_device_map}\")\n",
    "        else:\n",
    "            self.device = next(self.model.parameters()).device\n",
    "            print(f\"âœ“ Model loaded on device: {self.device}\")\n",
    "\n",
    "        print(f\"âœ“ Model ready\")\n",
    "        return self\n",
    "\n",
    "    def generate(self, prompt: str, temperature: float = None) -> str:\n",
    "        \"\"\"Generate response with specified temperature.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load()\n",
    "\n",
    "        temp = temperature if temperature is not None else Config.BASE_TEMPERATURE\n",
    "\n",
    "        # Format as chat\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        # Move to appropriate device (device_map requires explicit device placement)\n",
    "        if hasattr(self.model, \"hf_device_map\"):\n",
    "            # Model is split across GPUs, move inputs to cuda:0\n",
    "            inputs = inputs.to(\"cuda:0\")\n",
    "        else:\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=Config.MAX_NEW_TOKENS,\n",
    "                temperature=temp,\n",
    "                top_p=Config.TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "\n",
    "# Global model manager (lazy loading)\n",
    "model_manager = MultiGPUModelManager()\n",
    "print(\"âœ“ Multi-GPU model manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific solving instructions\n",
    "DOMAIN_INSTRUCTIONS = {\n",
    "    \"algebra\": \"\"\"For this algebra problem:\n",
    "- Identify key variables and equations\n",
    "- Use substitution, elimination, or factorization\n",
    "- Check for special cases and edge conditions\n",
    "- Verify your solution satisfies the original equation\"\"\",\n",
    "    \"geometry\": \"\"\"For this geometry problem:\n",
    "- Use pure synthetic reasoning (theorems, not coordinates)\n",
    "- Apply circle theorems, triangle properties, angle chasing\n",
    "- Look for cyclic quadrilaterals, power of a point\n",
    "- Draw auxiliary lines when helpful\"\"\",\n",
    "    \"combinatorics\": \"\"\"For this combinatorics problem:\n",
    "- Identify if counting, probability, or existence\n",
    "- Consider permutations, combinations, inclusion-exclusion\n",
    "- Look for symmetries and bijections\n",
    "- Check small cases first to identify patterns\"\"\",\n",
    "    \"number_theory\": \"\"\"For this number theory problem:\n",
    "- Consider divisibility, prime factorization, modular arithmetic\n",
    "- Apply Fermat's Little Theorem, Euler's theorem, CRT\n",
    "- Look for patterns in residues modulo small primes\n",
    "- Use Euclidean algorithm for gcd/lcm\"\"\",\n",
    "    \"unknown\": \"Use general mathematical reasoning and careful step-by-step analysis.\",\n",
    "}\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AnalyzerAgent(BaseAgent):\n",
    "    \"\"\"Analyzes the problem to understand its domain and requirements.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Analyzer\")\n",
    "\n",
    "    def run(self, problem: str) -> Dict:\n",
    "        \"\"\"Analyze the problem and return structured information.\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"You are a mathematical problem analyzer for competition mathematics (IMO/AIME level).\n",
    "\n",
    "{REFERENCE_PROBLEMS}\n",
    "\n",
    "Now analyze this problem:\n",
    "Problem: {problem}\n",
    "\n",
    "You MUST respond with a valid Python dictionary in this EXACT format:\n",
    "{{\n",
    "    \"domain\": \"algebra\",\n",
    "    \"problem_type\": \"computation\",\n",
    "    \"difficulty_estimate\": \"hard\",\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\"],\n",
    "    \"suggested_approach\": \"brief description\"\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- Use DOUBLE QUOTES for all strings\n",
    "- Domain must be exactly: algebra, geometry, combinatorics, number_theory, or unknown\n",
    "- Provide 1-3 key concepts as a list\n",
    "- Keep suggested_approach brief (under 100 characters)\n",
    "\n",
    "Respond ONLY with the dictionary, no other text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model_manager.generate(prompt, temperature=0.1)\n",
    "\n",
    "            # Extract JSON from response\n",
    "            json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "            if json_match:\n",
    "                analysis = ast.literal_eval(json_match.group())\n",
    "                return analysis\n",
    "        except Exception as e:\n",
    "            print(f\"Analyzer error: {e}\")\n",
    "\n",
    "        # Fallback analysis\n",
    "        return {\n",
    "            \"domain\": \"unknown\",\n",
    "            \"problem_type\": \"computation\",\n",
    "            \"difficulty_estimate\": \"medium\",\n",
    "            \"key_concepts\": [],\n",
    "            \"suggested_approach\": \"General mathematical reasoning\",\n",
    "        }\n",
    "\n",
    "\n",
    "class SolverAgent(BaseAgent):\n",
    "    \"\"\"Generates solutions with chain-of-thought reasoning.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Solver\")\n",
    "\n",
    "    def create_prompt(self, problem: str, analysis: Dict, temperature_idx: int) -> str:\n",
    "        \"\"\"Create solving prompt with appropriate instructions.\"\"\"\n",
    "\n",
    "        domain = analysis.get(\"domain\", \"unknown\")\n",
    "        approach = analysis.get(\"suggested_approach\", \"\")\n",
    "        domain_instruction = DOMAIN_INSTRUCTIONS.get(\n",
    "            domain, DOMAIN_INSTRUCTIONS[\"unknown\"]\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"You are an expert mathematical problem solver specializing in competition mathematics (IMO/AIME level).\n",
    "\n",
    "{REFERENCE_PROBLEMS}\n",
    "\n",
    "Now solve this problem:\n",
    "Problem: {problem}\n",
    "\n",
    "Analysis: This is a {domain} problem. {approach}\n",
    "\n",
    "{domain_instruction}\n",
    "\n",
    "Important Instructions:\n",
    "1. Think step-by-step using chain-of-reasoning\n",
    "2. Show all your work clearly\n",
    "3. The answer MUST be a non-negative integer between 0 and 99999\n",
    "4. If the problem asks for a remainder, compute it correctly\n",
    "\n",
    "Work through the solution step by step.\n",
    "\n",
    "After solving, add a VERIFICATION section:\n",
    "- Double-check your calculations\n",
    "- Verify the answer satisfies all problem conditions\n",
    "- Confirm the answer is in the valid range (0-99999)\n",
    "\n",
    "State your final answer clearly as: FINAL ANSWER: [number]\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def extract_answer(self, response: str) -> Optional[int]:\n",
    "        \"\"\"Extract the integer answer from model response.\"\"\"\n",
    "\n",
    "        # Multiple patterns for answer extraction\n",
    "        patterns = [\n",
    "            r\"FINAL ANSWER:\\s*(\\d+)\",\n",
    "            r\"final answer is:?\\s*(\\d+)\",\n",
    "            r\"answer is:?\\s*(\\d+)\",\n",
    "            r\"the answer is:?\\s*(\\d+)\",\n",
    "            r\"\\\\boxed\\{(\\d+)\\}\",\n",
    "            r\"\\*\\*(\\d+)\\*\\*\",\n",
    "            r\"\\b(\\d{1,5})\\b(?!\\s*\\.\\d)\",  # Standalone numbers\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "            if matches:\n",
    "                # Get the last match (usually the final answer)\n",
    "                for match in reversed(matches):\n",
    "                    try:\n",
    "                        num = int(match)\n",
    "                        if Config.MIN_ANSWER <= num <= Config.MAX_ANSWER:\n",
    "                            return num\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    def run(self, problem: str, analysis: Dict, temp_idx: int) -> Dict:\n",
    "        \"\"\"Generate a solution with specified temperature.\"\"\"\n",
    "\n",
    "        temperature = Config.TEMPERATURES[temp_idx % len(Config.TEMPERATURES)]\n",
    "        prompt = self.create_prompt(problem, analysis, temp_idx)\n",
    "\n",
    "        try:\n",
    "            response = model_manager.generate(prompt, temperature=temperature)\n",
    "            answer = self.extract_answer(response)\n",
    "\n",
    "            return {\n",
    "                \"success\": answer is not None,\n",
    "                \"answer\": answer,\n",
    "                \"raw_response\": response,\n",
    "                \"temperature\": temperature,\n",
    "                \"temp_idx\": temp_idx,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"answer\": None,\n",
    "                \"error\": str(e),\n",
    "                \"temperature\": temperature,\n",
    "                \"temp_idx\": temp_idx,\n",
    "            }\n",
    "\n",
    "\n",
    "class ConsensusVotingAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Implements consensus voting across multiple solutions.\n",
    "    Generates N solutions and selects the majority answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConsensusVoting\")\n",
    "        self.analyzer = AnalyzerAgent()\n",
    "        self.solver = SolverAgent()\n",
    "\n",
    "    def run(self, problem: str) -> int:\n",
    "        \"\"\"\n",
    "        Solve problem using consensus voting.\n",
    "        Returns the majority answer or best available.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"CONSENSUS VOTING SOLVER\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        # Step 1: Analyze problem\n",
    "        print(\"Step 1: Analyzing problem...\")\n",
    "        analysis = self.analyzer.run(problem)\n",
    "        domain = analysis.get(\"domain\", \"unknown\")\n",
    "        print(f\"  Domain: {domain}\")\n",
    "        print(f\"  Approach: {analysis.get('suggested_approach', 'N/A')}\")\n",
    "\n",
    "        # Step 2: Generate multiple solutions\n",
    "        print(f\"\\nStep 2: Generating {Config.NUM_SOLUTIONS} solutions...\")\n",
    "        solutions = []\n",
    "\n",
    "        for i in range(Config.NUM_SOLUTIONS):\n",
    "            print(\n",
    "                f\"  Solution {i + 1}/{Config.NUM_SOLUTIONS} (temp={Config.TEMPERATURES[i]})...\"\n",
    "            )\n",
    "\n",
    "            result = self.solver.run(problem, analysis, i)\n",
    "            solutions.append(result)\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                print(f\"    âœ“ Answer: {result['answer']}\")\n",
    "            else:\n",
    "                print(f\"    âœ— Failed: {result.get('error', 'Unknown')}\")\n",
    "\n",
    "        # Step 3: Consensus voting\n",
    "        print(f\"\\nStep 3: Consensus voting...\")\n",
    "\n",
    "        # Collect all valid answers\n",
    "        valid_answers = [\n",
    "            s[\"answer\"] for s in solutions if s[\"success\"] and s[\"answer\"] is not None\n",
    "        ]\n",
    "\n",
    "        if not valid_answers:\n",
    "            print(\"  No valid answers! Returning 0\")\n",
    "            return 0\n",
    "\n",
    "        # Count votes\n",
    "        answer_counts = Counter(valid_answers)\n",
    "        print(f\"  Vote distribution: {dict(answer_counts)}\")\n",
    "\n",
    "        # Get majority\n",
    "        majority_answer, vote_count = answer_counts.most_common(1)[0]\n",
    "\n",
    "        # Check if we have a clear majority or tie\n",
    "        if vote_count > len(valid_answers) / 2:\n",
    "            print(\n",
    "                f\"  âœ“ Clear majority: {majority_answer} ({vote_count}/{len(valid_answers)} votes)\"\n",
    "            )\n",
    "        else:\n",
    "            # Tie or no clear majority - pick most common\n",
    "            print(\n",
    "                f\"  ~ Plurality winner: {majority_answer} ({vote_count}/{len(valid_answers)} votes)\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nFinal answer: {majority_answer}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        return majority_answer\n",
    "\n",
    "\n",
    "print(\"âœ“ All agent classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Solver Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "    \"\"\"Main solver with consensus voting.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.consensus_agent = ConsensusVotingAgent()\n",
    "\n",
    "    def solve(self, problem: str) -> int:\n",
    "        \"\"\"Solve a single problem.\"\"\"\n",
    "        try:\n",
    "            answer = self.consensus_agent.run(problem)\n",
    "            # Ensure valid range\n",
    "            return max(Config.MIN_ANSWER, min(Config.MAX_ANSWER, int(answer)))\n",
    "        except Exception as e:\n",
    "            print(f\"Critical error: {e}\")\n",
    "            return 0\n",
    "\n",
    "\n",
    "# Global solver (lazy loaded)\n",
    "_solver = None\n",
    "\n",
    "\n",
    "def get_solver():\n",
    "    \"\"\"Get or create solver instance.\"\"\"\n",
    "    global _solver\n",
    "    if _solver is None:\n",
    "        _solver = AIMO3Solver()\n",
    "    return _solver\n",
    "\n",
    "\n",
    "print(\"âœ“ Main solver interface ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Inference Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Kaggle inference API function.\n",
    "\n",
    "    Args:\n",
    "        id_: Polars Series containing problem ID\n",
    "        problem: Polars Series containing problem text\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with 'id' and 'answer' columns\n",
    "    \"\"\"\n",
    "    # Unpack values\n",
    "    problem_id = id_.item(0)\n",
    "    problem_text = problem.item(0)\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing: {problem_id}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Problem: {problem_text[:100]}...\")\n",
    "\n",
    "    # Get solver and solve\n",
    "    solver = get_solver()\n",
    "    answer = solver.solve(problem_text)\n",
    "\n",
    "    print(f\"âœ“ Answer: {answer}\")\n",
    "\n",
    "    # Return as DataFrame\n",
    "    return pl.DataFrame({\"id\": problem_id, \"answer\": answer})\n",
    "\n",
    "\n",
    "print(\"âœ“ Predict function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AIMO3 Consensus Voting Solver\")\n",
    "print(\"Optimized for 2x T4 GPUs\")\n",
    "print(\"Using offline quantized Kaggle dataset model (17GB)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize inference server\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "# Check if running in competition mode\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    print(\"\\nðŸš€ Starting inference server in PRODUCTION mode...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"\\nðŸ§ª Running in LOCAL TEST mode...\")\n",
    "\n",
    "    # Try to run on test data\n",
    "    test_path = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n",
    "\n",
    "    if os.path.exists(test_path):\n",
    "        print(f\"Loading test data from: {test_path}\")\n",
    "        inference_server.run_local_gateway((test_path,))\n",
    "    else:\n",
    "        print(\"Test file not found. Running sample problem...\")\n",
    "\n",
    "        # Test with a simple problem\n",
    "        sample_problem = (\n",
    "            \"What is the sum of all positive integers $n$ such that $n^2 - 3n + 2 = 0$?\"\n",
    "        )\n",
    "\n",
    "        test_id = pl.Series([\"test001\"])\n",
    "        test_problem = pl.Series([sample_problem])\n",
    "\n",
    "        result = predict(test_id, test_problem)\n",
    "        print(f\"\\nResult:\")\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
    "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "data_sources": [
    {
     "source_id": "gmhost/qwen2-5-32b-instruct-quant",
     "source_type": "dataset_version"
    }
   ],
   "docker_image_version_id": 30699,
   "is_gpu_enabled": true,
   "is_internet_enabled": false,
   "language": "python",
   "source_type": "notebook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}